# ğŸ§ª Profiling Results â€“ Basic Implementation (Global Memory)

This section breaks down the performance of the **basic CUDA convolution** version in **simple terms**.

--- 


## âš™ï¸ Overview

### âœ… What This Version Does:

- Uses **global memory** for both the input image and the filter.
- Each GPU thread is responsible for calculating **one pixel** in the output image.

### ğŸ‘‡ What Each Thread Does:

1. Reads **9 pixels** from the image (a 3Ã—3 block around the target pixel).
2. Reads **9 values** from the filter (Gaussian blur kernel).
3. Multiplies each pixel with its corresponding filter value and sums them up.
4. Writes the final blurred value into the output image.

â¡ï¸ Thatâ€™s a total of **19 memory operations per thread**:  
- 18 reads (9 from image + 9 from filter)  
- 1 write to the output image

---

## ğŸ§¨ Why It's Slow (In Simple Words)

Even though the math is fast, the problem is **memory**.

### 1. ğŸ§µ Threads Access Memory Inefficiently

- Threads work on neighboring pixels, but the **memory pattern is scattered**.
- GPUs are fast when threads read memory **together** (coalesced), but this 2D pattern doesnâ€™t always allow that.
- This causes the GPU to **spend more time waiting** for data from memory.

### 2. ğŸ” Threads Repeat Work

- Threads often read the **same pixels** as their neighbors (because 3x3 blocks overlap).
- But in global memory, there's **no sharing**â€”so they read the same thing again and again.
- This wastes memory bandwidth and slows things down.

### 3. ğŸ§‚ Filter Is Small, But Still Slow

- All threads use the **same 3x3 filter**, but each one reads it from global memory.
- Thatâ€™s unnecessary! This data could be kept in **faster memory** (like constant memory), but isnâ€™t in this version.

---

## â›” Bottlenecks

| Problem                     | Why It Matters                                  |
|----------------------------|--------------------------------------------------|
| Global Memory is Slow      | GPU memory (global) is slower than shared/constant |
| Reads Are Redundant        | Many threads read the same image pixels          |
| Access Pattern Is Scattered | Makes it harder for the GPU to fetch data quickly |
| Filter Reads Are Repeated  | All threads read the same filter separately      |

---

## ğŸ“‰ Performance Numbers

- **Execution Time:** 1.074 milliseconds
- **Memory Bandwidth Used:** 26.480 GB/s  
â¡ï¸ Much lower than the GPU's potential (e.g., 300+ GB/s)

### Why?  
Because most of the time is spent **waiting for memory**, not doing actual math.

---

## ğŸ§  Final Thoughts

This version is simple, but not efficient.  
It shows how memory access patternsâ€”**not just math**â€”can make or break GPU performance.  
Thatâ€™s why further optimizations (like constant and shared memory) are important ğŸ”§ğŸ’¡

---
